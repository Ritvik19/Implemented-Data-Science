{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c34887c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.015711,
     "end_time": "2022-01-14T08:17:04.790332",
     "exception": false,
     "start_time": "2022-01-14T08:17:04.774621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Normalization in Neural Networks\n",
    "\n",
    "In this notebook I'll discuss the different types of Normalizations that are commonly used in Neural Networks, along with their applications and implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b602b0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T08:17:04.827831Z",
     "iopub.status.busy": "2022-01-14T08:17:04.826143Z",
     "iopub.status.idle": "2022-01-14T08:17:10.526012Z",
     "shell.execute_reply": "2022-01-14T08:17:10.524825Z",
     "shell.execute_reply.started": "2022-01-14T05:40:05.919725Z"
    },
    "papermill": {
     "duration": 5.719227,
     "end_time": "2022-01-14T08:17:10.526206",
     "exception": false,
     "start_time": "2022-01-14T08:17:04.806979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf6884",
   "metadata": {
    "papermill": {
     "duration": 0.015061,
     "end_time": "2022-01-14T08:17:10.555982",
     "exception": false,
     "start_time": "2022-01-14T08:17:10.540921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Batch Normalization\n",
    "\n",
    "Batch Normalization was first discussed in the paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf)\n",
    "\n",
    "They define Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training. This adversely affects training speed because the later layers have to adapt to the shifted distribution.\n",
    "\n",
    "They proposed that by whitening the inputs to each layer,we would take a step towards achieving the fixed distributions of inputs that would remove the ill effects of the internal covariate shift.\n",
    "\n",
    "Whitening is linearly transforming inputs to have zero mean, unit variance, and be uncorrelated.\n",
    "\n",
    "**The paper introduces Batch Normalization as follows:**\n",
    "\n",
    "1. Normalize each feature independently to have zero mean and unit variance:\n",
    "<center><h3>$$ \\hat{x}^{(k)} = \\frac{x^{(k)} - E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}} $$</h3></center>\n",
    "where $ x = (x^{(1)}...x^{(d)}) $ is the d-dimensional input.\n",
    "2. The estimates of mean and variance are from the mini-batch for normalization; instead of calculating the mean and variance across the whole dataset.\n",
    "3. Normalizing each feature to zero mean and unit variance could affect what the layer can represent. To overcome this each feature is scaled and shifted by two trained parameters.\n",
    "<center><h3>$$ y^{(k)} = \\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)} $$</h3></center>\n",
    "where $ y^{(k)} $ is the output of the batch normalization layer.\n",
    "4. An exponential moving average of mean and variance is calculated during the training phase and is then used during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc01ae26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T08:17:10.597358Z",
     "iopub.status.busy": "2022-01-14T08:17:10.596733Z",
     "iopub.status.idle": "2022-01-14T08:17:10.598751Z",
     "shell.execute_reply": "2022-01-14T08:17:10.599174Z",
     "shell.execute_reply.started": "2022-01-14T06:17:29.109194Z"
    },
    "papermill": {
     "duration": 0.028681,
     "end_time": "2022-01-14T08:17:10.599334",
     "exception": false,
     "start_time": "2022-01-14T08:17:10.570653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BatchNormalization(L.Layer):\n",
    "    def __init__(self, eps=1e-5, momentum=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum   \n",
    "    \n",
    "    def call(self, x):\n",
    "        x_shape = x.shape\n",
    "        batch_size = x_shape[0]\n",
    "        channels = x.shape[-1]\n",
    "        \n",
    "        self.exp_mean = tf.zeros(channels)\n",
    "        self.exp_var = tf.ones(channels)\n",
    "        \n",
    "        self.scale = tf.zeros(channels)\n",
    "        self.shift = tf.ones(channels)\n",
    "        \n",
    "        x = tf.reshape(x, (batch_size, -1, channels))\n",
    "        \n",
    "        mean = tf.reduce_mean(x, [0, 1])\n",
    "        mean_x2 = tf.reduce_mean((x ** 2), [0, 1])\n",
    "        var = mean_x2 - mean ** 2\n",
    "        \n",
    "        self.exp_mean = (1 - self.momentum) * self.exp_mean + self.momentum * mean\n",
    "        self.exp_var = (1 - self.momentum) * self.exp_var + self.momentum * var\n",
    "        \n",
    "        mean = self.exp_mean\n",
    "        var = self.exp_var\n",
    "        \n",
    "        x_norm = (x - tf.reshape(mean, (1, 1, -1))) / tf.reshape(tf.sqrt(var + self.eps), (1, 1, -1))\n",
    "        x_norm = tf.reshape(self.scale, (1, 1, -1)) * x_norm +tf.reshape(self.shift, (1, 1, -1))\n",
    "        return tf.reshape(x_norm, x_shape)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "189b29ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T08:17:10.638292Z",
     "iopub.status.busy": "2022-01-14T08:17:10.637637Z",
     "iopub.status.idle": "2022-01-14T08:17:10.746664Z",
     "shell.execute_reply": "2022-01-14T08:17:10.747166Z",
     "shell.execute_reply.started": "2022-01-14T06:18:13.135417Z"
    },
    "papermill": {
     "duration": 0.133255,
     "end_time": "2022-01-14T08:17:10.747331",
     "exception": false,
     "start_time": "2022-01-14T08:17:10.614076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "User settings:\n",
      "\n",
      "   KMP_AFFINITY=granularity=fine,verbose,compact,1,0\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_SETTINGS=1\n",
      "   KMP_WARNINGS=0\n",
      "\n",
      "Effective settings:\n",
      "\n",
      "   KMP_ABORT_DELAY=0\n",
      "   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n",
      "   KMP_ALIGN_ALLOC=64\n",
      "   KMP_ALL_THREADPRIVATE=128\n",
      "   KMP_ATOMIC_MODE=2\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_CPUINFO_FILE: value is not defined\n",
      "   KMP_DETERMINISTIC_REDUCTION=false\n",
      "   KMP_DEVICE_THREAD_LIMIT=2147483647\n",
      "   KMP_DISP_NUM_BUFFERS=7\n",
      "   KMP_DUPLICATE_LIB_OK=false\n",
      "   KMP_ENABLE_TASK_THROTTLING=true\n",
      "   KMP_FORCE_REDUCTION: value is not defined\n",
      "   KMP_FOREIGN_THREADS_THREADPRIVATE=true\n",
      "   KMP_FORKJOIN_BARRIER='2,2'\n",
      "   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_GTID_MODE=3\n",
      "   KMP_HANDLE_SIGNALS=false\n",
      "   KMP_HOT_TEAMS_MAX_LEVEL=1\n",
      "   KMP_HOT_TEAMS_MODE=0\n",
      "   KMP_INIT_AT_FORK=true\n",
      "   KMP_LIBRARY=throughput\n",
      "   KMP_LOCK_KIND=queuing\n",
      "   KMP_MALLOC_POOL_INCR=1M\n",
      "   KMP_NUM_LOCKS_IN_BLOCK=1\n",
      "   KMP_PLAIN_BARRIER='2,2'\n",
      "   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_REDUCTION_BARRIER='1,1'\n",
      "   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_SCHEDULE='static,balanced;guided,iterative'\n",
      "   KMP_SETTINGS=true\n",
      "   KMP_SPIN_BACKOFF_PARAMS='4096,100'\n",
      "   KMP_STACKOFFSET=64\n",
      "   KMP_STACKPAD=0\n",
      "   KMP_STACKSIZE=8M\n",
      "   KMP_STORAGE_MAP=false\n",
      "   KMP_TASKING=2\n",
      "   KMP_TASKLOOP_MIN_TASKS=0\n",
      "   KMP_TASK_STEALING_CONSTRAINT=1\n",
      "   KMP_TEAMS_THREAD_LIMIT=4\n",
      "   KMP_TOPOLOGY_METHOD=all\n",
      "   KMP_USE_YIELD=1\n",
      "   KMP_VERSION=false\n",
      "   KMP_WARNINGS=false\n",
      "   OMP_AFFINITY_FORMAT='OMP: pid %P tid %i thread %n bound to OS proc set {%A}'\n",
      "   OMP_ALLOCATOR=omp_default_mem_alloc\n",
      "   OMP_CANCELLATION=false\n",
      "   OMP_DEFAULT_DEVICE=0\n",
      "   OMP_DISPLAY_AFFINITY=false\n",
      "   OMP_DISPLAY_ENV=false\n",
      "   OMP_DYNAMIC=false\n",
      "   OMP_MAX_ACTIVE_LEVELS=1\n",
      "   OMP_MAX_TASK_PRIORITY=0\n",
      "   OMP_NESTED: deprecated; max-active-levels-var=1\n",
      "   OMP_NUM_THREADS: value is not defined\n",
      "   OMP_PLACES: value is not defined\n",
      "   OMP_PROC_BIND='intel'\n",
      "   OMP_SCHEDULE='static'\n",
      "   OMP_STACKSIZE=8M\n",
      "   OMP_TARGET_OFFLOAD=DEFAULT\n",
      "   OMP_THREAD_LIMIT=2147483647\n",
      "   OMP_WAIT_POLICY=PASSIVE\n",
      "   KMP_AFFINITY='verbose,warnings,respect,granularity=fine,compact,1,0'\n",
      "\n",
      "2022-01-14 08:17:10.653484: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal((32, 24, 24, 3))\n",
    "assert BatchNormalization()(x).shape == x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76d4c3d",
   "metadata": {
    "papermill": {
     "duration": 0.014405,
     "end_time": "2022-01-14T08:17:10.777070",
     "exception": false,
     "start_time": "2022-01-14T08:17:10.762665",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Layer Normalization\n",
    "Layer normalization, introduced in the paper [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf) is a simpler normalization method that is generally used for NLP tasks but works on a wider range of settings.\n",
    "\n",
    "<center><h3>$ LN(x) = \\gamma . \\frac{X - E_{H, W, C}[X]}{\\sqrt{Var_{H, W, C}[x] + \\epsilon}} + \\beta $</h3></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aae7c1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T08:17:10.809958Z",
     "iopub.status.busy": "2022-01-14T08:17:10.809347Z",
     "iopub.status.idle": "2022-01-14T08:17:10.816777Z",
     "shell.execute_reply": "2022-01-14T08:17:10.817348Z",
     "shell.execute_reply.started": "2022-01-14T06:35:24.172191Z"
    },
    "papermill": {
     "duration": 0.025496,
     "end_time": "2022-01-14T08:17:10.817530",
     "exception": false,
     "start_time": "2022-01-14T08:17:10.792034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerNormalization(L.Layer):\n",
    "    def __init__(self, eps=1e-5,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def call(self, x):\n",
    "        normalized_shape = x.shape[1:]\n",
    "                \n",
    "        self.gain = tf.zeros(normalized_shape)\n",
    "        self.bias = tf.ones(normalized_shape)\n",
    "        \n",
    "        dims = [-(i + 1) for i in range(len(normalized_shape))]\n",
    "          \n",
    "        mean = tf.reduce_mean(x, dims, keepdims=True)\n",
    "        mean_x2 = tf.reduce_mean((x**2), dims, keepdims=True)\n",
    "        var = mean_x2 - mean ** 2\n",
    "        \n",
    "        x_norm = (x - mean) / tf.sqrt(var + self.eps)\n",
    "        x_norm = self.gain * x_norm + self.bias\n",
    "\n",
    "        return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc272d57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T08:17:10.850827Z",
     "iopub.status.busy": "2022-01-14T08:17:10.850192Z",
     "iopub.status.idle": "2022-01-14T08:17:10.861866Z",
     "shell.execute_reply": "2022-01-14T08:17:10.861208Z",
     "shell.execute_reply.started": "2022-01-14T06:35:24.583255Z"
    },
    "papermill": {
     "duration": 0.02956,
     "end_time": "2022-01-14T08:17:10.862003",
     "exception": false,
     "start_time": "2022-01-14T08:17:10.832443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = tf.random.normal((32, 24, 24, 3))\n",
    "assert LayerNormalization()(x).shape == x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed326d2",
   "metadata": {
    "papermill": {
     "duration": 0.014955,
     "end_time": "2022-01-14T08:17:10.892009",
     "exception": false,
     "start_time": "2022-01-14T08:17:10.877054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Instance Normalization\n",
    "Instance normalization was introduced in the paper [Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/pdf/1607.08022.pdf) to improve style transfer.\n",
    "\n",
    "<center><h3>$ IN(x) = \\gamma . \\frac{X - E_{H, W}[X]}{\\sqrt{Var_{H, W}[x] + \\epsilon}} + \\beta $</h3></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "263bd28e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T08:17:10.933047Z",
     "iopub.status.busy": "2022-01-14T08:17:10.926129Z",
     "iopub.status.idle": "2022-01-14T08:17:10.935458Z",
     "shell.execute_reply": "2022-01-14T08:17:10.934942Z",
     "shell.execute_reply.started": "2022-01-14T06:52:47.603725Z"
    },
    "papermill": {
     "duration": 0.027746,
     "end_time": "2022-01-14T08:17:10.935602",
     "exception": false,
     "start_time": "2022-01-14T08:17:10.907856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InstanceNormalization(L.Layer):\n",
    "    def __init__(self, eps=1e-5,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def call(self, x):\n",
    "        x_shape = x.shape\n",
    "        batch_size = x_shape[0]\n",
    "        channels = x.shape[-1]\n",
    "        \n",
    "        self.scale = tf.zeros(channels)\n",
    "        self.shift = tf.ones(channels)\n",
    "        \n",
    "        x = tf.reshape(x, (batch_size, -1, channels))\n",
    "        \n",
    "        mean = tf.reduce_mean(x, [1], keepdims=True)\n",
    "        mean_x2 = tf.reduce_mean((x ** 2), [1], keepdims=True)\n",
    "        var = mean_x2 - mean ** 2\n",
    "        \n",
    "        x_norm = (x - mean) / tf.sqrt(var + self.eps)\n",
    "        x_norm = tf.reshape(self.scale, (1, 1, -1)) * x_norm +tf.reshape(self.shift, (1, 1, -1))\n",
    "        \n",
    "        return tf.reshape(x_norm, x_shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69e53ee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T08:17:10.970883Z",
     "iopub.status.busy": "2022-01-14T08:17:10.970199Z",
     "iopub.status.idle": "2022-01-14T08:17:10.982770Z",
     "shell.execute_reply": "2022-01-14T08:17:10.983314Z",
     "shell.execute_reply.started": "2022-01-14T06:52:47.981872Z"
    },
    "papermill": {
     "duration": 0.032762,
     "end_time": "2022-01-14T08:17:10.983482",
     "exception": false,
     "start_time": "2022-01-14T08:17:10.950720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = tf.random.normal((32, 24, 24, 3))\n",
    "assert InstanceNormalization()(x).shape == x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb7eb37",
   "metadata": {
    "papermill": {
     "duration": 0.014625,
     "end_time": "2022-01-14T08:17:11.013421",
     "exception": false,
     "start_time": "2022-01-14T08:17:10.998796",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Group Normalization\n",
    "Batch Normalization works well for large enough batch sizes but not well for small batch sizes, because it normalizes over the batch. Training large models with large batch sizes is not possible due to the memory capacity of the devices.\n",
    "\n",
    "Group Normalization introduced in the paper [Group Normalization](https://arxiv.org/pdf/1803.08494.pdf), normalizes a set of features together as a group. This is based on the observation that classical features such as SIFT and HOG are group-wise features. The paper proposes dividing feature channels into groups and then separately normalizing all channels within each group.\n",
    "\n",
    "All normalization layers can be defined by the following computation.\n",
    "<center><h3>$ \\hat{x}_{i} = \\frac{1}{\\sigma_{i}}(x_{i} - \\mu_{i}) $</h3></center>\n",
    "where $ \\mu_{i} $ and $ \\sigma_{i} $ are mean and standard deviation\n",
    "<center><h3>$ \\mu_{i}  = \\frac{1}{m}\\Sigma_{k \\in S_{i}} x_{k} $</h3></center>\n",
    "<center><h3>$ \\sigma_{i}  = \\sqrt{\\frac{1}{m}\\Sigma_{k \\in S_{i}} (x_{k} - \\mu_{i})^{2} + \\epsilon} $</h3></center>\n",
    "$ S_{i} $ is the set of indexes across which the mean and standard deviation are calculated for index i. m is the size of the set $ S_{i} $ which is the same for all i.\n",
    "\n",
    "The definition of $ S_{i} $ is different for Batch normalization, Layer normalization, and Instance normalization.\n",
    "\n",
    "\n",
    "**Batch Normalization**\n",
    "<center><h3> $ S_{i} = \\{ k|k_{c} = i_{c} \\} $ </h3></center>\n",
    "The values that share the same feature channel are normalized together.\n",
    "<br><br>\n",
    "\n",
    "**Layer Normalization**\n",
    "<center><h3> $ S_{i} = \\{ k|k_{n} = i_{n} \\} $ </h3></center>\n",
    "The values from the same sample in the batch are normalized together.\n",
    "<br><br>\n",
    "\n",
    "**Instance Normalization**\n",
    "<center><h3> $ S_{i} = \\{ k|k_{n} = i_{n}, k_{c} = i_{c} \\} $ </h3></center>\n",
    "The values from the same sample and same feature channel are normalized together.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Group Normalization**\n",
    "<center><h3> $ S_{i} = \\{ k|k_{n} = i_{n}, floor(\\frac{k_{c}}{C/G}) = floor(\\frac{i_{c}}{C/G}) \\} $ </h3></center>\n",
    "where $ G $ is the number of groups and $ C $ is the number of channels.\n",
    "\n",
    "Group normalization normalizes values of the same sample and the same group of channels together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03e1a944",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T08:17:11.047627Z",
     "iopub.status.busy": "2022-01-14T08:17:11.047025Z",
     "iopub.status.idle": "2022-01-14T08:17:11.056114Z",
     "shell.execute_reply": "2022-01-14T08:17:11.056637Z",
     "shell.execute_reply.started": "2022-01-14T07:38:25.858600Z"
    },
    "papermill": {
     "duration": 0.027268,
     "end_time": "2022-01-14T08:17:11.056806",
     "exception": false,
     "start_time": "2022-01-14T08:17:11.029538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GroupNormalization(L.Layer):\n",
    "    def __init__(self, groups, channels, eps=1e-5,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "        self.groups = groups\n",
    "        self.channels = channels\n",
    "        \n",
    "        self.scale = tf.zeros(channels)\n",
    "        self.shift = tf.ones(channels)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x_shape = x.shape\n",
    "        batch_size = x_shape[0]\n",
    "        \n",
    "        x = tf.reshape(x, (batch_size, -1, self.groups))\n",
    "        \n",
    "        mean = tf.reduce_mean(x, [1], keepdims=True)\n",
    "        mean_x2 = tf.reduce_mean((x ** 2), [1], keepdims=True)\n",
    "        var = mean_x2 - mean ** 2\n",
    "        \n",
    "        x_norm = (x - mean) / tf.sqrt(var + self.eps)\n",
    "        x_norm = tf.reshape(x_norm, (batch_size, -1, self.channels))\n",
    "        x_norm = tf.reshape(self.scale, (1, 1, -1)) * x_norm +tf.reshape(self.shift, (1, 1, -1))\n",
    "        \n",
    "        return tf.reshape(x_norm, x_shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "916f6927",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T08:17:11.092706Z",
     "iopub.status.busy": "2022-01-14T08:17:11.091752Z",
     "iopub.status.idle": "2022-01-14T08:17:11.104333Z",
     "shell.execute_reply": "2022-01-14T08:17:11.104930Z",
     "shell.execute_reply.started": "2022-01-14T07:38:26.572000Z"
    },
    "papermill": {
     "duration": 0.033251,
     "end_time": "2022-01-14T08:17:11.105094",
     "exception": false,
     "start_time": "2022-01-14T08:17:11.071843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = tf.random.normal((32, 24, 24, 8))\n",
    "assert GroupNormalization(2, 8)(x).shape == x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345ec253",
   "metadata": {
    "papermill": {
     "duration": 0.014968,
     "end_time": "2022-01-14T08:17:11.135171",
     "exception": false,
     "start_time": "2022-01-14T08:17:11.120203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Weight Standardization\n",
    "Batch Normalization doesn't work well when the batch size is too small, which happens when training large networks because of device memory limitations. The paper [Micro-Batch Training with Batch-Channel Normalization and Weight Standardization](https://arxiv.org/pdf/1903.10520.pdf) introduces Weight Standardization with Batch-Channel Normalization as a better alternative.\n",
    "\n",
    "<center><h3> $ \\hat{W_{i, j}} = \\frac{W_{i, j} - \\mu w_{i, .}}{ \\sigma w_{i, .} } $ </h3></center>\n",
    "where\n",
    "<center><h3> $ W \\in  R^{O \\times I} $ </h3></center>\n",
    "<center><h3> $ \\mu w_{i, .} = \\frac{1}{I} \\Sigma_{j=1}^{I} W_{i, j} $ </h3></center>\n",
    "<center><h3> $  \\sigma w_{i, .} = \\sqrt{\\frac{1}{I} \\Sigma_{j=1}^{I} W_{i, j}^{2} - \\mu w_{i, .}^{2} + \\epsilon} $ </h3></center>\n",
    "for a 2D-convolution layer $ O $ is the number of output channels and $ I $ is the number of input channels times the kernel size $ (I = C_{in} \\times K_{H} \\times K_{W}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a72f79cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T08:17:11.168883Z",
     "iopub.status.busy": "2022-01-14T08:17:11.168252Z",
     "iopub.status.idle": "2022-01-14T08:17:11.173465Z",
     "shell.execute_reply": "2022-01-14T08:17:11.174059Z",
     "shell.execute_reply.started": "2022-01-14T08:02:19.733807Z"
    },
    "papermill": {
     "duration": 0.023666,
     "end_time": "2022-01-14T08:17:11.174211",
     "exception": false,
     "start_time": "2022-01-14T08:17:11.150545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weight_standardization(weight):\n",
    "    c_out, c_in, *kernel_shape = weight.shape\n",
    "    weight = tf.reshape(weight, (c_out, -1))\n",
    "    \n",
    "    mean = tf.reduce_mean(weight, [1], keepdims=True)\n",
    "    mean_x2 = tf.reduce_mean((weight ** 2), [1], keepdims=True)\n",
    "    var = mean_x2 - mean ** 2\n",
    "    \n",
    "    weight = (weight - mean) / (tf.sqrt(var + eps))\n",
    "    \n",
    "    return tf.reshape(weight, (c_out, c_in, *kernel_shape)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed48040a",
   "metadata": {
    "papermill": {
     "duration": 0.01475,
     "end_time": "2022-01-14T08:17:11.203788",
     "exception": false,
     "start_time": "2022-01-14T08:17:11.189038",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Batch-Channel Normalization\n",
    "\n",
    "This first performs a batch normalizationThis first performs a batch normalization. Then a channel normalization is performed.\n",
    "\n",
    "Channel Normalization is similar to Group Normalization but affine transform is done group wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7028826b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T08:17:11.237401Z",
     "iopub.status.busy": "2022-01-14T08:17:11.236797Z",
     "iopub.status.idle": "2022-01-14T08:17:11.245113Z",
     "shell.execute_reply": "2022-01-14T08:17:11.245702Z",
     "shell.execute_reply.started": "2022-01-14T08:05:05.154168Z"
    },
    "papermill": {
     "duration": 0.026807,
     "end_time": "2022-01-14T08:17:11.245859",
     "exception": false,
     "start_time": "2022-01-14T08:17:11.219052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ChannelNormalization(L.Layer):\n",
    "    def __init__(self, groups, channels, eps=1e-5,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "        self.groups = groups\n",
    "        self.channels = channels\n",
    "        \n",
    "        self.scale = tf.zeros(groups)\n",
    "        self.shift = tf.ones(groups)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x_shape = x.shape\n",
    "        batch_size = x_shape[0]\n",
    "        \n",
    "        x = tf.reshape(x, (batch_size, -1, self.groups))\n",
    "        \n",
    "        mean = tf.reduce_mean(x, [1], keepdims=True)\n",
    "        mean_x2 = tf.reduce_mean((x ** 2), [1], keepdims=True)\n",
    "        var = mean_x2 - mean ** 2\n",
    "        \n",
    "        x_norm = (x - mean) / tf.sqrt(var + self.eps)\n",
    "#         x_norm = tf.reshape(x_norm, (batch_size, -1, self.channels))\n",
    "        x_norm = tf.reshape(self.scale, (1, 1, -1)) * x_norm +tf.reshape(self.shift, (1, 1, -1))\n",
    "        \n",
    "        return tf.reshape(x_norm, x_shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e0203be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T08:17:11.279466Z",
     "iopub.status.busy": "2022-01-14T08:17:11.278882Z",
     "iopub.status.idle": "2022-01-14T08:17:11.293372Z",
     "shell.execute_reply": "2022-01-14T08:17:11.293885Z",
     "shell.execute_reply.started": "2022-01-14T08:05:20.691079Z"
    },
    "papermill": {
     "duration": 0.033058,
     "end_time": "2022-01-14T08:17:11.294045",
     "exception": false,
     "start_time": "2022-01-14T08:17:11.260987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = tf.random.normal((32, 24, 24, 8))\n",
    "assert ChannelNormalization(2, 8)(x).shape == x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18afbad1",
   "metadata": {
    "papermill": {
     "duration": 0.014622,
     "end_time": "2022-01-14T08:17:11.325901",
     "exception": false,
     "start_time": "2022-01-14T08:17:11.311279",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# References\n",
    "1. [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf)\n",
    "2. [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)\n",
    "3. [Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/pdf/1607.08022.pdf)\n",
    "4. [Group Normalization](https://arxiv.org/pdf/1803.08494.pdf)\n",
    "5. [Micro-Batch Training with Batch-Channel Normalization and Weight Standardization](https://arxiv.org/pdf/1903.10520.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.919753,
   "end_time": "2022-01-14T08:17:14.947628",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-14T08:16:55.027875",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
