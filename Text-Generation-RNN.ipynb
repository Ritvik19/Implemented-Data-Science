{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from pylab import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = 20, 6\n",
    "rcParams['axes.grid'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(983, 68)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Game Of Thrones S01E01 Winter Is Coming.srt</th>\n",
       "      <th>Game Of Thrones S01E02 The Kingsroad.srt</th>\n",
       "      <th>Game Of Thrones S01E03 Lord Snow.srt</th>\n",
       "      <th>Game Of Thrones S01E04 Cripples, Bastards, And Broken Things.srt</th>\n",
       "      <th>Game Of Thrones S01E05 The Wolf And The Lion.srt</th>\n",
       "      <th>Game Of Thrones S01E06 A Golden Crown.srt</th>\n",
       "      <th>Game Of Thrones S01E07 You Win Or You Die.srt</th>\n",
       "      <th>Game Of Thrones S01E08 The Pointy End.srt</th>\n",
       "      <th>Game Of Thrones S01E09 Baelor.srt</th>\n",
       "      <th>Game Of Thrones S01E10 Fire And Blood.srt</th>\n",
       "      <th>...</th>\n",
       "      <th>Game Of Thrones S06E08 No One.srt</th>\n",
       "      <th>Game Of Thrones S06E09 Battle of the Bastards.srt</th>\n",
       "      <th>Game Of Thrones S06E10 The Winds of Winter.srt</th>\n",
       "      <th>Game Of Thrones S07E01 Dragonstone.srt</th>\n",
       "      <th>Game Of Thrones S07E02 Stormborn.srt</th>\n",
       "      <th>Game Of Thrones S07E03 The Queen's Justice.srt</th>\n",
       "      <th>Game Of Thrones S07E04 The Spoils Of War.srt</th>\n",
       "      <th>Game Of Thrones S07E05 Eastwatch.srt</th>\n",
       "      <th>Game Of Thrones S07E06 Beyond The Wall.srt</th>\n",
       "      <th>Game Of Thrones S07E07 The Dragon And The Wolf.srt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Easy, boy.</td>\n",
       "      <td>You need to drink, child.</td>\n",
       "      <td>Welcome, Lord Stark.</td>\n",
       "      <td>The little lord's been dreaming again.</td>\n",
       "      <td>Does Ser Hugh have any family in the capital?</td>\n",
       "      <td>Your pardon, Your Grace.</td>\n",
       "      <td>\"Summoned to court to answer for the crimes</td>\n",
       "      <td>Yah! Left high, left low.</td>\n",
       "      <td>You've seen better days, my lord.</td>\n",
       "      <td>Look at me. Look at me!</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;b&gt;&lt;font color=\"#00FF00\"&gt;♪ Game of Thrones 6x0...</td>\n",
       "      <td>Despite appearances, I think you'll find the c...</td>\n",
       "      <td>Your Grace.</td>\n",
       "      <td>♪ (PIANO PLAYING) ♪</td>\n",
       "      <td>ELLARIA: The Lannisters have declared war on h...</td>\n",
       "      <td>I wish you all the happiness in the world.</td>\n",
       "      <td>(GRUNTING)</td>\n",
       "      <td>Clegane, look into the flames.</td>\n",
       "      <td>LORD VARYS: Your father has proved to be an aw...</td>\n",
       "      <td>(GROANS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What do you expect? They're savages.</td>\n",
       "      <td>And eat.</td>\n",
       "      <td>Grand Maester Pycelle has called a meeting of ...</td>\n",
       "      <td>- We have visitors. - I don't want to see anyone.</td>\n",
       "      <td>No.</td>\n",
       "      <td>I would rise, but...</td>\n",
       "      <td>\"of your bannerman Gregor Clegane, the Mountai...</td>\n",
       "      <td>Right low, lunge right.</td>\n",
       "      <td>Another visit?</td>\n",
       "      <td>Do you remember me now, boy, eh?</td>\n",
       "      <td>...</td>\n",
       "      <td>== sync, corrected by &lt;font color=\"#00FF00\"&gt;el...</td>\n",
       "      <td>Perhaps we should take shelter.</td>\n",
       "      <td>The trial will be getting under way soon.</td>\n",
       "      <td>♪ (PIANO STOPS) ♪</td>\n",
       "      <td>They have declared war on Dorne.</td>\n",
       "      <td>- JAIME LANNISTER: Myrcella? - (PANTING)</td>\n",
       "      <td>I lost this dagger.</td>\n",
       "      <td>What do you see?</td>\n",
       "      <td>The girl is innocent. She should be given a ch...</td>\n",
       "      <td>Little Theon! Come and get her.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One lot steals a goat from another lot,</td>\n",
       "      <td>Isn't there anything else?</td>\n",
       "      <td>The honour of your presence is requested.</td>\n",
       "      <td>Really? If I was cooped up all day</td>\n",
       "      <td>I stood vigil for him myself last night.</td>\n",
       "      <td>Do you know what your wife has done?</td>\n",
       "      <td>\"Arrive within the fortnight or be branded an ...</td>\n",
       "      <td>You break anything, the septa will have my head.</td>\n",
       "      <td>It seems you're my last friend.</td>\n",
       "      <td>Remember me? There's a bright boy.</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;i&gt;My son.&lt;/i&gt;</td>\n",
       "      <td>The city's on the rise?</td>\n",
       "      <td>You got my money?</td>\n",
       "      <td>- (EXPLOSIONS) - (MAN YELLING)</td>\n",
       "      <td>We must be allies now, if we wish to survive.</td>\n",
       "      <td>Myrcella?</td>\n",
       "      <td>To whom?</td>\n",
       "      <td>THE HOUND: A wall of ice.</td>\n",
       "      <td>to prove her loyalty.</td>\n",
       "      <td>(CACKLES MANICALLY)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>before you know it they're ripping each other ...</td>\n",
       "      <td>The Dothraki have two things in abundance - gr...</td>\n",
       "      <td>Get the girls settled in. I'll be back in time...</td>\n",
       "      <td>with no one but this old bat for company, I'd ...</td>\n",
       "      <td>He had no one else.</td>\n",
       "      <td>- She did nothing I did not command. - Who'd h...</td>\n",
       "      <td>Poor Ned Stark.</td>\n",
       "      <td>What is...</td>\n",
       "      <td>No, no, many still love you.</td>\n",
       "      <td>You'll be coming with me, boy, and you'll be k...</td>\n",
       "      <td>...</td>\n",
       "      <td>My firstborn son.</td>\n",
       "      <td>Meereen is strong.</td>\n",
       "      <td>Later. Go away.</td>\n",
       "      <td>♪ (PIANO PLAYING) ♪</td>\n",
       "      <td>I offer vengeance... justice...</td>\n",
       "      <td>(SCREAMS)</td>\n",
       "      <td>Tyrion Lannister.</td>\n",
       "      <td>The Wall.</td>\n",
       "      <td>CERSEI LANNISTER: You must write to Lady Catelyn.</td>\n",
       "      <td>It's an invitation... to King's Landing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I've never seen wildlings do a thing like this.</td>\n",
       "      <td>People can't live on grass.</td>\n",
       "      <td>- And, Jory, you go with them. - Yes, my lord.</td>\n",
       "      <td>Anyway, you don't have a choice. Robb's waiting.</td>\n",
       "      <td>He'd never worn this armour before.</td>\n",
       "      <td>By what right dare you lay hands on my blood?</td>\n",
       "      <td>Brave man, terrible judgment.</td>\n",
       "      <td>Your sister knew perfectly well we were to lea...</td>\n",
       "      <td>Sansa came to court this morning to plead for ...</td>\n",
       "      <td>- Keep your mouth shut, boy. - I'm not a boy!</td>\n",
       "      <td>...</td>\n",
       "      <td>My child king, hush.</td>\n",
       "      <td>Commerce has returned to the markets.</td>\n",
       "      <td>Grand Maester.</td>\n",
       "      <td>The war is over. Winter has come.</td>\n",
       "      <td>Fire and blood.</td>\n",
       "      <td>Kill us!</td>\n",
       "      <td>Take a step toward a more productive relationship</td>\n",
       "      <td>The only thing standing between us</td>\n",
       "      <td>If you would help your father, urge your brother</td>\n",
       "      <td>You will represent my interests</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Game Of Thrones S01E01 Winter Is Coming.srt  \\\n",
       "1                                         Easy, boy.   \n",
       "2               What do you expect? They're savages.   \n",
       "3            One lot steals a goat from another lot,   \n",
       "4  before you know it they're ripping each other ...   \n",
       "5    I've never seen wildlings do a thing like this.   \n",
       "\n",
       "            Game Of Thrones S01E02 The Kingsroad.srt  \\\n",
       "1                          You need to drink, child.   \n",
       "2                                           And eat.   \n",
       "3                         Isn't there anything else?   \n",
       "4  The Dothraki have two things in abundance - gr...   \n",
       "5                        People can't live on grass.   \n",
       "\n",
       "                Game Of Thrones S01E03 Lord Snow.srt  \\\n",
       "1                               Welcome, Lord Stark.   \n",
       "2  Grand Maester Pycelle has called a meeting of ...   \n",
       "3          The honour of your presence is requested.   \n",
       "4  Get the girls settled in. I'll be back in time...   \n",
       "5     - And, Jory, you go with them. - Yes, my lord.   \n",
       "\n",
       "  Game Of Thrones S01E04 Cripples, Bastards, And Broken Things.srt  \\\n",
       "1             The little lord's been dreaming again.                 \n",
       "2  - We have visitors. - I don't want to see anyone.                 \n",
       "3                 Really? If I was cooped up all day                 \n",
       "4  with no one but this old bat for company, I'd ...                 \n",
       "5   Anyway, you don't have a choice. Robb's waiting.                 \n",
       "\n",
       "  Game Of Thrones S01E05 The Wolf And The Lion.srt  \\\n",
       "1    Does Ser Hugh have any family in the capital?   \n",
       "2                                              No.   \n",
       "3         I stood vigil for him myself last night.   \n",
       "4                              He had no one else.   \n",
       "5              He'd never worn this armour before.   \n",
       "\n",
       "           Game Of Thrones S01E06 A Golden Crown.srt  \\\n",
       "1                           Your pardon, Your Grace.   \n",
       "2                               I would rise, but...   \n",
       "3               Do you know what your wife has done?   \n",
       "4  - She did nothing I did not command. - Who'd h...   \n",
       "5      By what right dare you lay hands on my blood?   \n",
       "\n",
       "       Game Of Thrones S01E07 You Win Or You Die.srt  \\\n",
       "1        \"Summoned to court to answer for the crimes   \n",
       "2  \"of your bannerman Gregor Clegane, the Mountai...   \n",
       "3  \"Arrive within the fortnight or be branded an ...   \n",
       "4                                    Poor Ned Stark.   \n",
       "5                      Brave man, terrible judgment.   \n",
       "\n",
       "           Game Of Thrones S01E08 The Pointy End.srt  \\\n",
       "1                          Yah! Left high, left low.   \n",
       "2                            Right low, lunge right.   \n",
       "3   You break anything, the septa will have my head.   \n",
       "4                                         What is...   \n",
       "5  Your sister knew perfectly well we were to lea...   \n",
       "\n",
       "                   Game Of Thrones S01E09 Baelor.srt  \\\n",
       "1                  You've seen better days, my lord.   \n",
       "2                                     Another visit?   \n",
       "3                    It seems you're my last friend.   \n",
       "4                       No, no, many still love you.   \n",
       "5  Sansa came to court this morning to plead for ...   \n",
       "\n",
       "           Game Of Thrones S01E10 Fire And Blood.srt  ...  \\\n",
       "1                            Look at me. Look at me!  ...   \n",
       "2                   Do you remember me now, boy, eh?  ...   \n",
       "3                 Remember me? There's a bright boy.  ...   \n",
       "4  You'll be coming with me, boy, and you'll be k...  ...   \n",
       "5      - Keep your mouth shut, boy. - I'm not a boy!  ...   \n",
       "\n",
       "                   Game Of Thrones S06E08 No One.srt  \\\n",
       "1  <b><font color=\"#00FF00\">♪ Game of Thrones 6x0...   \n",
       "2  == sync, corrected by <font color=\"#00FF00\">el...   \n",
       "3                                     <i>My son.</i>   \n",
       "4                                  My firstborn son.   \n",
       "5                               My child king, hush.   \n",
       "\n",
       "   Game Of Thrones S06E09 Battle of the Bastards.srt  \\\n",
       "1  Despite appearances, I think you'll find the c...   \n",
       "2                    Perhaps we should take shelter.   \n",
       "3                            The city's on the rise?   \n",
       "4                                 Meereen is strong.   \n",
       "5              Commerce has returned to the markets.   \n",
       "\n",
       "  Game Of Thrones S06E10 The Winds of Winter.srt  \\\n",
       "1                                    Your Grace.   \n",
       "2      The trial will be getting under way soon.   \n",
       "3                              You got my money?   \n",
       "4                                Later. Go away.   \n",
       "5                                 Grand Maester.   \n",
       "\n",
       "  Game Of Thrones S07E01 Dragonstone.srt  \\\n",
       "1                    ♪ (PIANO PLAYING) ♪   \n",
       "2                      ♪ (PIANO STOPS) ♪   \n",
       "3         - (EXPLOSIONS) - (MAN YELLING)   \n",
       "4                    ♪ (PIANO PLAYING) ♪   \n",
       "5      The war is over. Winter has come.   \n",
       "\n",
       "                Game Of Thrones S07E02 Stormborn.srt  \\\n",
       "1  ELLARIA: The Lannisters have declared war on h...   \n",
       "2                   They have declared war on Dorne.   \n",
       "3      We must be allies now, if we wish to survive.   \n",
       "4                    I offer vengeance... justice...   \n",
       "5                                    Fire and blood.   \n",
       "\n",
       "  Game Of Thrones S07E03 The Queen's Justice.srt  \\\n",
       "1     I wish you all the happiness in the world.   \n",
       "2       - JAIME LANNISTER: Myrcella? - (PANTING)   \n",
       "3                                      Myrcella?   \n",
       "4                                      (SCREAMS)   \n",
       "5                                       Kill us!   \n",
       "\n",
       "        Game Of Thrones S07E04 The Spoils Of War.srt  \\\n",
       "1                                         (GRUNTING)   \n",
       "2                                I lost this dagger.   \n",
       "3                                           To whom?   \n",
       "4                                  Tyrion Lannister.   \n",
       "5  Take a step toward a more productive relationship   \n",
       "\n",
       "  Game Of Thrones S07E05 Eastwatch.srt  \\\n",
       "1       Clegane, look into the flames.   \n",
       "2                     What do you see?   \n",
       "3            THE HOUND: A wall of ice.   \n",
       "4                            The Wall.   \n",
       "5   The only thing standing between us   \n",
       "\n",
       "          Game Of Thrones S07E06 Beyond The Wall.srt  \\\n",
       "1  LORD VARYS: Your father has proved to be an aw...   \n",
       "2  The girl is innocent. She should be given a ch...   \n",
       "3                              to prove her loyalty.   \n",
       "4  CERSEI LANNISTER: You must write to Lady Catelyn.   \n",
       "5   If you would help your father, urge your brother   \n",
       "\n",
       "  Game Of Thrones S07E07 The Dragon And The Wolf.srt  \n",
       "1                                           (GROANS)  \n",
       "2                    Little Theon! Come and get her.  \n",
       "3                                (CACKLES MANICALLY)  \n",
       "4           It's an invitation... to King's Landing.  \n",
       "5                    You will represent my interests  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = [pd.read_json(f'../input/game-of-thrones-srt/season{i+1}.json') for i in range(7)]\n",
    "df = pd.concat(df, axis=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Easy, boy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You need to drink, child.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Welcome, Lord Stark.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The little lord's been dreaming again.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Does Ser Hugh have any family in the capital?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Script\n",
       "0                                     Easy, boy.\n",
       "1                      You need to drink, child.\n",
       "2                           Welcome, Lord Stark.\n",
       "3         The little lord's been dreaming again.\n",
       "4  Does Ser Hugh have any family in the capital?"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df.values.reshape(-1, 1), columns=['Script']).dropna().reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "  cleantext = re.sub(cleanr, '', raw_html)\n",
    "  return cleantext\n",
    "\n",
    "df['Script'] = df['Script'].apply(cleanhtml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 unique characters\n"
     ]
    }
   ],
   "source": [
    "text = '\\n'.join(df['Script'].values)\n",
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the text\n",
    "Before training, we need to map strings to a numerical representation. We create two lookup tables: one mapping characters to numbers, and another for numbers to characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The prediction task\n",
    "Given a character, or a sequence of characters, what is the most probable next character? This is the task we're training the model to perform. The input to the model will be a sequence of characters, and we train the model to predict the output—the following character at each time step.\n",
    "\n",
    "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n",
    "\n",
    "## Create training examples and targets\n",
    "Next we divide the text into example sequences. Each input sequence will contain seq_length characters from the text.\n",
    "\n",
    "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
    "\n",
    "So break the text into chunks of seq_length+1. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
    "\n",
    "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E\n",
      "a\n",
      "s\n",
      "y\n",
      ",\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch method lets us easily convert these individual characters to sequences of the desired size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Easy, boy.\\nYou need to drink, child.\\nWelcome, Lord Stark.\\nThe little lord's been dreaming again.\\nDoes\"\n",
      "' Ser Hugh have any family in the capital?\\nYour pardon, Your Grace.\\n\"Summoned to court to answer for t'\n",
      "\"he crimes\\nYah! Left high, left low.\\nYou've seen better days, my lord.\\nLook at me. Look at me!\\nWell st\"\n",
      "\"ruck.\\nGotta be ready before nightfall.\\nOut, all of you.\\nIt's got to be the Mountain. He's the biggest\"\n",
      "\".\\n- You swear it? - By the mother.\\nI've taken your castle.\\nA cripple?\\n- Riders approaching! - Open th\"\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sequence, duplicate and shift it to form the input and target text by using the map method to apply a simple function to each batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  \"Easy, boy.\\nYou need to drink, child.\\nWelcome, Lord Stark.\\nThe little lord's been dreaming again.\\nDoe\"\n",
      "Target data: \"asy, boy.\\nYou need to drink, child.\\nWelcome, Lord Stark.\\nThe little lord's been dreaming again.\\nDoes\"\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each index of these vectors is processed as a one time step. For the input at time step 0, the model receives the index for \"E\" and tries to predict the index for \"a\" as the next character. At the next timestep, it does the same thing but the RNN considers the previous step context in addition to the current input character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 32 ('E')\n",
      "  expected output: 56 ('a')\n",
      "Step    1\n",
      "  input: 56 ('a')\n",
      "  expected output: 74 ('s')\n",
      "Step    2\n",
      "  input: 74 ('s')\n",
      "  expected output: 80 ('y')\n",
      "Step    3\n",
      "  input: 80 ('y')\n",
      "  expected output: 9 (',')\n",
      "Step    4\n",
      "  input: 9 (',')\n",
      "  expected output: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training batches\n",
    "\n",
    "We used tf.data to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build The Model\n",
    "\n",
    "We'll use `keras.model.Sequential` to define the model. For this simple example three types of layers are used to define our model:\n",
    "\n",
    "* `keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with embedding_dim dimensions;\n",
    "* `keras.layers.GRU`: A type of RNN with size units=rnn_units (`keras.layers.LSTM` and `keras.layers.SimpleRNN` can also be used)\n",
    "* `keras.layers.Dense`: The output layer, with vocab_size outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = ['G1024']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense \n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=10)\n",
    "filepath = \"model.h5\"\n",
    "ckpt = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min', save_weights_only=True)\n",
    "rlp = ReduceLROnPlateau(monitor='loss', patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]))\n",
    "    for rnn_unit in rnn_units:\n",
    "        layer_type = rnn_unit[0]\n",
    "        num_cells = int(rnn_unit[1:])\n",
    "        if layer_type == 'G':\n",
    "            model.add(GRU(\n",
    "                num_cells, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'\n",
    "            ))\n",
    "        elif layer_type == 'L':\n",
    "            model.add(LSTM(\n",
    "                num_cells, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'\n",
    "            ))\n",
    "        else:\n",
    "            model.add(SimpleRNN(\n",
    "                num_cells, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'\n",
    "            ))\n",
    "\n",
    "    model.add(Dense(vocab_size))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n",
    "\n",
    "![](https://www.tensorflow.org/tutorials/text/images/text_generation_training.png)\n",
    "\n",
    "Please note that Keras sequential model is used here since all the layers in the model only have single input and produce single output. In case you want to retrieve and reuse the states from stateful RNN layer, you might want to build your model with Keras functional API or model subclassing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           22016     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 86)            88150     \n",
      "=================================================================\n",
      "Total params: 4,048,470\n",
      "Trainable params: 4,048,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the model\n",
    "Now run the model to see that it behaves as expected.\n",
    "\n",
    "First check the shape of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 86) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
    "\n",
    "Try it for the first example in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us, at each timestep, a prediction of the next character index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([76, 44,  5, 67, 79,  8, 83, 11,  1, 31, 69, 28,  2, 43, 15,  7, 77,\n",
       "       40,  8, 22, 53, 21, 32, 22, 46, 37, 58,  7,  4, 67, 41, 56, 44, 38,\n",
       "       75, 30, 16, 57, 22, 34, 50, 38, 22, 62, 48, 26, 55, 35, 39, 62, 55,\n",
       "       72, 11, 81, 79, 39, 12, 23, 49,  9, 69, 57, 74, 45, 77, 27, 45, 68,\n",
       "       28, 36, 67, 36, 67, 56, 77, 66, 52, 50,  1,  8, 64,  8, 54, 79, 70,\n",
       "       69, 58, 25,  9, 36, 76, 14,  1,  3,  6, 65, 66, 44, 51, 55])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " ' of battle.\\nAnd I am an excellent judge of character.\\nTogether, we will save this country\\nI almost d'\n",
      "\n",
      "Next Char Predictions: \n",
      " 'uQ&lx)í. DnA!P3(vM):Z9E:SJc(#lNaQKtC4b:GWK:gU?`HLg`q.zxL0;V,nbsRv@RmAIlIlavkYW )i)_xonc>,Iu2 \"\\'jkQX`'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character.\n",
    "\n",
    "## Attach an optimizer, and a loss function\n",
    "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
    "\n",
    "Because your model returns logits, you need to set the from_logits flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 86)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.45373\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the training procedure using the `tf.keras.Model.compile` method. Use `tf.keras.optimizers.Adam` with default arguments and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 2.4689\n",
      "Epoch 00001: loss improved from inf to 2.46673, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 2.4667\n",
      "Epoch 2/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 1.7472\n",
      "Epoch 00002: loss improved from 2.46673 to 1.74640, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 1.7464\n",
      "Epoch 3/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 1.4711\n",
      "Epoch 00003: loss improved from 1.74640 to 1.47077, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 1.4708\n",
      "Epoch 4/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 1.3473\n",
      "Epoch 00004: loss improved from 1.47077 to 1.34729, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 1.3473\n",
      "Epoch 5/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 1.2783\n",
      "Epoch 00005: loss improved from 1.34729 to 1.27826, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 1.2783\n",
      "Epoch 6/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 1.2295\n",
      "Epoch 00006: loss improved from 1.27826 to 1.22956, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 1.2296\n",
      "Epoch 7/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 1.1920\n",
      "Epoch 00007: loss improved from 1.22956 to 1.19211, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 1.1921\n",
      "Epoch 8/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 1.1599\n",
      "Epoch 00008: loss improved from 1.19211 to 1.15995, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 1.1600\n",
      "Epoch 9/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 1.1311\n",
      "Epoch 00009: loss improved from 1.15995 to 1.13113, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 1.1311\n",
      "Epoch 10/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 1.1026\n",
      "Epoch 00010: loss improved from 1.13113 to 1.10264, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 1.1026\n",
      "Epoch 11/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 1.0746\n",
      "Epoch 00011: loss improved from 1.10264 to 1.07460, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 1.0746\n",
      "Epoch 12/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 1.0487\n",
      "Epoch 00012: loss improved from 1.07460 to 1.04872, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 1.0487\n",
      "Epoch 13/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 1.0218\n",
      "Epoch 00013: loss improved from 1.04872 to 1.02182, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 1.0218\n",
      "Epoch 14/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.9943\n",
      "Epoch 00014: loss improved from 1.02182 to 0.99427, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.9943\n",
      "Epoch 15/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.9684\n",
      "Epoch 00015: loss improved from 0.99427 to 0.96842, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.9684\n",
      "Epoch 16/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.9423\n",
      "Epoch 00016: loss improved from 0.96842 to 0.94232, saving model to model.h5\n",
      "232/232 [==============================] - 11s 47ms/step - loss: 0.9423\n",
      "Epoch 17/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.9166\n",
      "Epoch 00017: loss improved from 0.94232 to 0.91659, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.9166\n",
      "Epoch 18/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.8936\n",
      "Epoch 00018: loss improved from 0.91659 to 0.89365, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.8937\n",
      "Epoch 19/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.8709\n",
      "Epoch 00019: loss improved from 0.89365 to 0.87100, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.8710\n",
      "Epoch 20/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.8518\n",
      "Epoch 00020: loss improved from 0.87100 to 0.85182, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.8518\n",
      "Epoch 21/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.8339\n",
      "Epoch 00021: loss improved from 0.85182 to 0.83411, saving model to model.h5\n",
      "232/232 [==============================] - 11s 47ms/step - loss: 0.8341\n",
      "Epoch 22/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.8183\n",
      "Epoch 00022: loss improved from 0.83411 to 0.81833, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.8183\n",
      "Epoch 23/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.8039\n",
      "Epoch 00023: loss improved from 0.81833 to 0.80389, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.8039\n",
      "Epoch 24/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.7925\n",
      "Epoch 00024: loss improved from 0.80389 to 0.79248, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.7925\n",
      "Epoch 25/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.7815\n",
      "Epoch 00025: loss improved from 0.79248 to 0.78152, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.7815\n",
      "Epoch 26/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.7739\n",
      "Epoch 00026: loss improved from 0.78152 to 0.77410, saving model to model.h5\n",
      "232/232 [==============================] - 11s 47ms/step - loss: 0.7741\n",
      "Epoch 27/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.7669\n",
      "Epoch 00027: loss improved from 0.77410 to 0.76692, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.7669\n",
      "Epoch 28/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.7595\n",
      "Epoch 00028: loss improved from 0.76692 to 0.75950, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.7595\n",
      "Epoch 29/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.7542\n",
      "Epoch 00029: loss improved from 0.75950 to 0.75415, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.7542\n",
      "Epoch 30/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.7497\n",
      "Epoch 00030: loss improved from 0.75415 to 0.74991, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.7499\n",
      "Epoch 31/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.7489\n",
      "Epoch 00031: loss improved from 0.74991 to 0.74892, saving model to model.h5\n",
      "232/232 [==============================] - 11s 47ms/step - loss: 0.7489\n",
      "Epoch 32/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.7475\n",
      "Epoch 00032: loss improved from 0.74892 to 0.74769, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.7477\n",
      "Epoch 33/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.7440\n",
      "Epoch 00033: loss improved from 0.74769 to 0.74402, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.7440\n",
      "Epoch 34/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.7434\n",
      "Epoch 00034: loss improved from 0.74402 to 0.74340, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.7434\n",
      "Epoch 35/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.7432\n",
      "Epoch 00035: loss improved from 0.74340 to 0.74320, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.7432\n",
      "Epoch 36/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.7411\n",
      "Epoch 00036: loss improved from 0.74320 to 0.74109, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.7411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.7480\n",
      "Epoch 00037: loss did not improve from 0.74109\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.7482\n",
      "Epoch 38/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.7486\n",
      "Epoch 00038: loss did not improve from 0.74109\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.7488\n",
      "Epoch 39/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.7487\n",
      "Epoch 00039: loss did not improve from 0.74109\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.7488\n",
      "Epoch 40/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.6525\n",
      "Epoch 00040: loss improved from 0.74109 to 0.65240, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.6524\n",
      "Epoch 41/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.5910\n",
      "Epoch 00041: loss improved from 0.65240 to 0.59098, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.5910\n",
      "Epoch 42/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.5628\n",
      "Epoch 00042: loss improved from 0.59098 to 0.56285, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.5628\n",
      "Epoch 43/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.5442\n",
      "Epoch 00043: loss improved from 0.56285 to 0.54415, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.5442\n",
      "Epoch 44/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.5289\n",
      "Epoch 00044: loss improved from 0.54415 to 0.52889, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.5289\n",
      "Epoch 45/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.5165\n",
      "Epoch 00045: loss improved from 0.52889 to 0.51646, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.5165\n",
      "Epoch 46/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.5060\n",
      "Epoch 00046: loss improved from 0.51646 to 0.50602, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.5060\n",
      "Epoch 47/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.4976\n",
      "Epoch 00047: loss improved from 0.50602 to 0.49761, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.4976\n",
      "Epoch 48/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.4890\n",
      "Epoch 00048: loss improved from 0.49761 to 0.48899, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.4890\n",
      "Epoch 49/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.4818\n",
      "Epoch 00049: loss improved from 0.48899 to 0.48177, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.4818\n",
      "Epoch 50/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.4751\n",
      "Epoch 00050: loss improved from 0.48177 to 0.47513, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.4751\n",
      "Epoch 51/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.4692\n",
      "Epoch 00051: loss improved from 0.47513 to 0.46912, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.4691\n",
      "Epoch 52/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.4627\n",
      "Epoch 00052: loss improved from 0.46912 to 0.46283, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.4628\n",
      "Epoch 53/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.4567\n",
      "Epoch 00053: loss improved from 0.46283 to 0.45669, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.4567\n",
      "Epoch 54/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.4527\n",
      "Epoch 00054: loss improved from 0.45669 to 0.45274, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.4527\n",
      "Epoch 55/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.4472\n",
      "Epoch 00055: loss improved from 0.45274 to 0.44721, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.4472\n",
      "Epoch 56/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.4431\n",
      "Epoch 00056: loss improved from 0.44721 to 0.44309, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.4431\n",
      "Epoch 57/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.4387\n",
      "Epoch 00057: loss improved from 0.44309 to 0.43868, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.4387\n",
      "Epoch 58/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.4344\n",
      "Epoch 00058: loss improved from 0.43868 to 0.43444, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.4344\n",
      "Epoch 59/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.4317\n",
      "Epoch 00059: loss improved from 0.43444 to 0.43174, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.4317\n",
      "Epoch 60/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.4268\n",
      "Epoch 00060: loss improved from 0.43174 to 0.42683, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.4268\n",
      "Epoch 61/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.4239\n",
      "Epoch 00061: loss improved from 0.42683 to 0.42392, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.4239\n",
      "Epoch 62/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.4202\n",
      "Epoch 00062: loss improved from 0.42392 to 0.42024, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.4202\n",
      "Epoch 63/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.4178\n",
      "Epoch 00063: loss improved from 0.42024 to 0.41780, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.4178\n",
      "Epoch 64/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.4145\n",
      "Epoch 00064: loss improved from 0.41780 to 0.41448, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.4145\n",
      "Epoch 65/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.4119\n",
      "Epoch 00065: loss improved from 0.41448 to 0.41191, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.4119\n",
      "Epoch 66/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.4088\n",
      "Epoch 00066: loss improved from 0.41191 to 0.40883, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.4088\n",
      "Epoch 67/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.4067\n",
      "Epoch 00067: loss improved from 0.40883 to 0.40665, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.4067\n",
      "Epoch 68/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.4041\n",
      "Epoch 00068: loss improved from 0.40665 to 0.40409, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.4041\n",
      "Epoch 69/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.4012\n",
      "Epoch 00069: loss improved from 0.40409 to 0.40133, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.4013\n",
      "Epoch 70/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3994\n",
      "Epoch 00070: loss improved from 0.40133 to 0.39938, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3994\n",
      "Epoch 71/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3972\n",
      "Epoch 00071: loss improved from 0.39938 to 0.39725, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.3972\n",
      "Epoch 72/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3952\n",
      "Epoch 00072: loss improved from 0.39725 to 0.39528, saving model to model.h5\n",
      "232/232 [==============================] - 11s 47ms/step - loss: 0.3953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3927\n",
      "Epoch 00073: loss improved from 0.39528 to 0.39273, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3927\n",
      "Epoch 74/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3912\n",
      "Epoch 00074: loss improved from 0.39273 to 0.39122, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3912\n",
      "Epoch 75/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3883\n",
      "Epoch 00075: loss improved from 0.39122 to 0.38828, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.3883\n",
      "Epoch 76/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3876\n",
      "Epoch 00076: loss improved from 0.38828 to 0.38761, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.3876\n",
      "Epoch 77/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3857\n",
      "Epoch 00077: loss improved from 0.38761 to 0.38572, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3857\n",
      "Epoch 78/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3843\n",
      "Epoch 00078: loss improved from 0.38572 to 0.38428, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3843\n",
      "Epoch 79/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3826\n",
      "Epoch 00079: loss improved from 0.38428 to 0.38261, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3826\n",
      "Epoch 80/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3816\n",
      "Epoch 00080: loss improved from 0.38261 to 0.38160, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3816\n",
      "Epoch 81/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3792\n",
      "Epoch 00081: loss improved from 0.38160 to 0.37918, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3792\n",
      "Epoch 82/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3778\n",
      "Epoch 00082: loss improved from 0.37918 to 0.37780, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3778\n",
      "Epoch 83/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3761\n",
      "Epoch 00083: loss improved from 0.37780 to 0.37607, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3761\n",
      "Epoch 84/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3742\n",
      "Epoch 00084: loss improved from 0.37607 to 0.37420, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3742\n",
      "Epoch 85/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3743\n",
      "Epoch 00085: loss did not improve from 0.37420\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3743\n",
      "Epoch 86/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3725\n",
      "Epoch 00086: loss improved from 0.37420 to 0.37254, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.3725\n",
      "Epoch 87/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3716\n",
      "Epoch 00087: loss improved from 0.37254 to 0.37164, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3716\n",
      "Epoch 88/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3696\n",
      "Epoch 00088: loss improved from 0.37164 to 0.36959, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3696\n",
      "Epoch 89/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3677\n",
      "Epoch 00089: loss improved from 0.36959 to 0.36770, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3677\n",
      "Epoch 90/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3669\n",
      "Epoch 00090: loss improved from 0.36770 to 0.36695, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3669\n",
      "Epoch 91/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3676\n",
      "Epoch 00091: loss did not improve from 0.36695\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3676\n",
      "Epoch 92/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3651\n",
      "Epoch 00092: loss improved from 0.36695 to 0.36511, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3651\n",
      "Epoch 93/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3638\n",
      "Epoch 00093: loss improved from 0.36511 to 0.36381, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3638\n",
      "Epoch 94/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3630\n",
      "Epoch 00094: loss improved from 0.36381 to 0.36305, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3630\n",
      "Epoch 95/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3619\n",
      "Epoch 00095: loss improved from 0.36305 to 0.36190, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3619\n",
      "Epoch 96/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3611\n",
      "Epoch 00096: loss improved from 0.36190 to 0.36108, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3611\n",
      "Epoch 97/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3608\n",
      "Epoch 00097: loss improved from 0.36108 to 0.36082, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3608\n",
      "Epoch 98/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3577\n",
      "Epoch 00098: loss improved from 0.36082 to 0.35773, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3577\n",
      "Epoch 99/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3581\n",
      "Epoch 00099: loss did not improve from 0.35773\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3580\n",
      "Epoch 100/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3568\n",
      "Epoch 00100: loss improved from 0.35773 to 0.35679, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3568\n",
      "Epoch 101/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3555\n",
      "Epoch 00101: loss improved from 0.35679 to 0.35551, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3555\n",
      "Epoch 102/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3546\n",
      "Epoch 00102: loss improved from 0.35551 to 0.35460, saving model to model.h5\n",
      "232/232 [==============================] - 11s 47ms/step - loss: 0.3546\n",
      "Epoch 103/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3531\n",
      "Epoch 00103: loss improved from 0.35460 to 0.35306, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3531\n",
      "Epoch 104/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3528\n",
      "Epoch 00104: loss improved from 0.35306 to 0.35290, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.3529\n",
      "Epoch 105/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3521\n",
      "Epoch 00105: loss improved from 0.35290 to 0.35212, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3521\n",
      "Epoch 106/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3522\n",
      "Epoch 00106: loss improved from 0.35212 to 0.35211, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3521\n",
      "Epoch 107/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3506\n",
      "Epoch 00107: loss improved from 0.35211 to 0.35063, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3506\n",
      "Epoch 108/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3492\n",
      "Epoch 00108: loss improved from 0.35063 to 0.34919, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3492\n",
      "Epoch 109/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3500\n",
      "Epoch 00109: loss did not improve from 0.34919\n",
      "232/232 [==============================] - 10s 44ms/step - loss: 0.3500\n",
      "Epoch 110/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3478\n",
      "Epoch 00110: loss improved from 0.34919 to 0.34785, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3478\n",
      "Epoch 111/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3483\n",
      "Epoch 00111: loss did not improve from 0.34785\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3483\n",
      "Epoch 112/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3474\n",
      "Epoch 00112: loss improved from 0.34785 to 0.34745, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3475\n",
      "Epoch 113/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3457\n",
      "Epoch 00113: loss improved from 0.34745 to 0.34566, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3457\n",
      "Epoch 114/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3459\n",
      "Epoch 00114: loss did not improve from 0.34566\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3459\n",
      "Epoch 115/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3455\n",
      "Epoch 00115: loss improved from 0.34566 to 0.34555, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3455\n",
      "Epoch 116/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3446\n",
      "Epoch 00116: loss improved from 0.34555 to 0.34460, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3446\n",
      "Epoch 117/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3427\n",
      "Epoch 00117: loss improved from 0.34460 to 0.34269, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3427\n",
      "Epoch 118/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3440\n",
      "Epoch 00118: loss did not improve from 0.34269\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3440\n",
      "Epoch 119/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3432\n",
      "Epoch 00119: loss did not improve from 0.34269\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3432\n",
      "Epoch 120/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3407\n",
      "Epoch 00120: loss improved from 0.34269 to 0.34068, saving model to model.h5\n",
      "232/232 [==============================] - 10s 44ms/step - loss: 0.3407\n",
      "Epoch 121/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3409\n",
      "Epoch 00121: loss did not improve from 0.34068\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3409\n",
      "Epoch 122/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3396\n",
      "Epoch 00122: loss improved from 0.34068 to 0.33958, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.3396\n",
      "Epoch 123/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3401\n",
      "Epoch 00123: loss did not improve from 0.33958\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3401\n",
      "Epoch 124/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3400\n",
      "Epoch 00124: loss did not improve from 0.33958\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3400\n",
      "Epoch 125/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3387\n",
      "Epoch 00125: loss improved from 0.33958 to 0.33872, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3387\n",
      "Epoch 126/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3379\n",
      "Epoch 00126: loss improved from 0.33872 to 0.33786, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.3379\n",
      "Epoch 127/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3375\n",
      "Epoch 00127: loss improved from 0.33786 to 0.33747, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3375\n",
      "Epoch 128/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3373\n",
      "Epoch 00128: loss improved from 0.33747 to 0.33726, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3373\n",
      "Epoch 129/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3369\n",
      "Epoch 00129: loss improved from 0.33726 to 0.33687, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3369\n",
      "Epoch 130/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3364\n",
      "Epoch 00130: loss improved from 0.33687 to 0.33642, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3364\n",
      "Epoch 131/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3358\n",
      "Epoch 00131: loss improved from 0.33642 to 0.33584, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3358\n",
      "Epoch 132/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3344\n",
      "Epoch 00132: loss improved from 0.33584 to 0.33442, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3344\n",
      "Epoch 133/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3343\n",
      "Epoch 00133: loss improved from 0.33442 to 0.33432, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.3343\n",
      "Epoch 134/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3329\n",
      "Epoch 00134: loss improved from 0.33432 to 0.33287, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.3329\n",
      "Epoch 135/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3336\n",
      "Epoch 00135: loss did not improve from 0.33287\n",
      "232/232 [==============================] - 10s 44ms/step - loss: 0.3336\n",
      "Epoch 136/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3326\n",
      "Epoch 00136: loss improved from 0.33287 to 0.33259, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3326\n",
      "Epoch 137/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3325\n",
      "Epoch 00137: loss improved from 0.33259 to 0.33248, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3325\n",
      "Epoch 138/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3290\n",
      "Epoch 00144: loss improved from 0.32936 to 0.32896, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3290\n",
      "Epoch 145/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3288\n",
      "Epoch 00145: loss improved from 0.32896 to 0.32876, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3288\n",
      "Epoch 146/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3280\n",
      "Epoch 00146: loss improved from 0.32876 to 0.32803, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3280\n",
      "Epoch 147/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3284\n",
      "Epoch 00147: loss did not improve from 0.32803\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.3284\n",
      "Epoch 148/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3274\n",
      "Epoch 00148: loss improved from 0.32803 to 0.32740, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3274\n",
      "Epoch 149/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3273\n",
      "Epoch 00149: loss improved from 0.32740 to 0.32727, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3273\n",
      "Epoch 150/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3270\n",
      "Epoch 00150: loss improved from 0.32727 to 0.32701, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3270\n",
      "Epoch 151/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3256\n",
      "Epoch 00151: loss improved from 0.32701 to 0.32559, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.3256\n",
      "Epoch 152/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231/232 [============================>.] - ETA: 0s - loss: 0.3257\n",
      "Epoch 00152: loss did not improve from 0.32559\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3257\n",
      "Epoch 153/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3251\n",
      "Epoch 00153: loss improved from 0.32559 to 0.32506, saving model to model.h5\n",
      "232/232 [==============================] - 11s 47ms/step - loss: 0.3251\n",
      "Epoch 154/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3256\n",
      "Epoch 00154: loss did not improve from 0.32506\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3255\n",
      "Epoch 155/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3249\n",
      "Epoch 00155: loss improved from 0.32506 to 0.32492, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3249\n",
      "Epoch 156/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3237\n",
      "Epoch 00156: loss improved from 0.32492 to 0.32371, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3237\n",
      "Epoch 157/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3241\n",
      "Epoch 00157: loss did not improve from 0.32371\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3241\n",
      "Epoch 158/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3230\n",
      "Epoch 00158: loss improved from 0.32371 to 0.32309, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3231\n",
      "Epoch 159/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3223\n",
      "Epoch 00159: loss improved from 0.32309 to 0.32235, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3224\n",
      "Epoch 160/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3230\n",
      "Epoch 00160: loss did not improve from 0.32235\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3230\n",
      "Epoch 161/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3228\n",
      "Epoch 00161: loss did not improve from 0.32235\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3228\n",
      "Epoch 162/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3215\n",
      "Epoch 00162: loss improved from 0.32235 to 0.32153, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3215\n",
      "Epoch 163/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3215\n",
      "Epoch 00163: loss improved from 0.32153 to 0.32151, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3215\n",
      "Epoch 164/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3215\n",
      "Epoch 00164: loss improved from 0.32151 to 0.32147, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3215\n",
      "Epoch 165/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3207\n",
      "Epoch 00165: loss improved from 0.32147 to 0.32066, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3207\n",
      "Epoch 166/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3198\n",
      "Epoch 00166: loss improved from 0.32066 to 0.31984, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.3198\n",
      "Epoch 167/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3206\n",
      "Epoch 00167: loss did not improve from 0.31984\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3206\n",
      "Epoch 168/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3197\n",
      "Epoch 00168: loss did not improve from 0.31984\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3199\n",
      "Epoch 169/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3189\n",
      "Epoch 00169: loss improved from 0.31984 to 0.31888, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3189\n",
      "Epoch 170/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3195\n",
      "Epoch 00170: loss did not improve from 0.31888\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3195\n",
      "Epoch 171/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3190\n",
      "Epoch 00171: loss did not improve from 0.31888\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3190\n",
      "Epoch 172/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3175\n",
      "Epoch 00172: loss improved from 0.31888 to 0.31755, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3176\n",
      "Epoch 173/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3190\n",
      "Epoch 00173: loss did not improve from 0.31755\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3190\n",
      "Epoch 174/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3176\n",
      "Epoch 00174: loss did not improve from 0.31755\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3176\n",
      "Epoch 175/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3176\n",
      "Epoch 00175: loss did not improve from 0.31755\n",
      "\n",
      "Epoch 00175: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3176\n",
      "Epoch 176/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3132\n",
      "Epoch 00176: loss improved from 0.31755 to 0.31321, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3132\n",
      "Epoch 177/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3094\n",
      "Epoch 00177: loss improved from 0.31321 to 0.30937, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3094\n",
      "Epoch 178/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3088\n",
      "Epoch 00178: loss improved from 0.30937 to 0.30884, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3088\n",
      "Epoch 179/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3080\n",
      "Epoch 00179: loss improved from 0.30884 to 0.30799, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3080\n",
      "Epoch 180/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3074\n",
      "Epoch 00180: loss improved from 0.30799 to 0.30740, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3074\n",
      "Epoch 181/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3060\n",
      "Epoch 00181: loss improved from 0.30740 to 0.30602, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3060\n",
      "Epoch 182/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3056\n",
      "Epoch 00182: loss improved from 0.30602 to 0.30550, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3055\n",
      "Epoch 183/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3058\n",
      "Epoch 00183: loss did not improve from 0.30550\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3058\n",
      "Epoch 184/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3049\n",
      "Epoch 00184: loss improved from 0.30550 to 0.30485, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.3049\n",
      "Epoch 185/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3049\n",
      "Epoch 00185: loss did not improve from 0.30485\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.3049\n",
      "Epoch 186/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3052\n",
      "Epoch 00186: loss did not improve from 0.30485\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3052\n",
      "Epoch 187/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3040\n",
      "Epoch 00187: loss improved from 0.30485 to 0.30399, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.3040\n",
      "Epoch 188/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3043\n",
      "Epoch 00188: loss did not improve from 0.30399\n",
      "232/232 [==============================] - 11s 47ms/step - loss: 0.3043\n",
      "Epoch 189/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3047\n",
      "Epoch 00189: loss did not improve from 0.30399\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3051\n",
      "Epoch 00190: loss did not improve from 0.30399\n",
      "\n",
      "Epoch 00190: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3050\n",
      "Epoch 191/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3038\n",
      "Epoch 00191: loss improved from 0.30399 to 0.30380, saving model to model.h5\n",
      "232/232 [==============================] - 11s 45ms/step - loss: 0.3038\n",
      "Epoch 192/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3034\n",
      "Epoch 00192: loss improved from 0.30380 to 0.30335, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3034\n",
      "Epoch 193/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3036\n",
      "Epoch 00193: loss did not improve from 0.30335\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3036\n",
      "Epoch 194/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3027\n",
      "Epoch 00194: loss improved from 0.30335 to 0.30272, saving model to model.h5\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3027\n",
      "Epoch 195/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3039\n",
      "Epoch 00195: loss did not improve from 0.30272\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3038\n",
      "Epoch 196/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3026\n",
      "Epoch 00196: loss improved from 0.30272 to 0.30257, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3026\n",
      "Epoch 197/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3034\n",
      "Epoch 00197: loss did not improve from 0.30257\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3034\n",
      "Epoch 198/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3021\n",
      "Epoch 00198: loss improved from 0.30257 to 0.30206, saving model to model.h5\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3021\n",
      "Epoch 199/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3035\n",
      "Epoch 00199: loss did not improve from 0.30206\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3036\n",
      "Epoch 200/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3035\n",
      "Epoch 00200: loss did not improve from 0.30206\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3035\n",
      "Epoch 201/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3029\n",
      "Epoch 00201: loss did not improve from 0.30206\n",
      "\n",
      "Epoch 00201: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3029\n",
      "Epoch 202/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3037\n",
      "Epoch 00202: loss did not improve from 0.30206\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3037\n",
      "Epoch 203/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3033\n",
      "Epoch 00203: loss did not improve from 0.30206\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 0.3033\n",
      "Epoch 204/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3029\n",
      "Epoch 00204: loss did not improve from 0.30206\n",
      "\n",
      "Epoch 00204: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3029\n",
      "Epoch 205/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3038\n",
      "Epoch 00205: loss did not improve from 0.30206\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3038\n",
      "Epoch 206/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3023\n",
      "Epoch 00206: loss did not improve from 0.30206\n",
      "232/232 [==============================] - 10s 44ms/step - loss: 0.3024\n",
      "Epoch 207/500\n",
      "231/232 [============================>.] - ETA: 0s - loss: 0.3031\n",
      "Epoch 00207: loss did not improve from 0.30206\n",
      "\n",
      "Epoch 00207: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3032\n",
      "Epoch 208/500\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3029\n",
      "Epoch 00208: loss did not improve from 0.30206\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 0.3029\n",
      "Epoch 00208: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[es, ckpt, rlp])\n",
    "pd.DataFrame(history.history)[['loss']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text\n",
    "\n",
    "The following code block generates the text:\n",
    "\n",
    "* Begin by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
    "\n",
    "* Get the prediction distribution of the next character using the start string and the RNN state.\n",
    "\n",
    "* Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
    "\n",
    "* The RNN state returned by the model is fed back into the model so that it now has more context, instead of only one character. After predicting the next character, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters.\n",
    "\n",
    "![](https://www.tensorflow.org/tutorials/text/images/text_generation_sampling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            22016     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 86)             88150     \n",
      "=================================================================\n",
      "Total params: 4,048,470\n",
      "Trainable params: 4,048,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(filepath)\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 1000\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperature results in more predictable text.\n",
    "    # Higher temperature results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seven Hells me.\n",
      "Go on, girls. Get your hands off your freedom.\n",
      "The white walkers\n",
      "Green girls have important to me.\n",
      "I'm not dead yet, you know.\n",
      "And he said this to you alive, we're our help.\n",
      "He couldn't know.\n",
      "And you s and took the Seven Kingdoms.\n",
      "And what is their sigil?\n",
      "Bastard, come here.\n",
      "I don't want what I was doing wash yourself.\n",
      "I'm not living.\n",
      "My mother does raise the wolf again.\n",
      "Viserys are dead.\n",
      "The whole shit isalf.\n",
      "Our queen knows it is a tortured liar.\n",
      "He's a dragon. I have something for you.\n",
      "Archmaester, I won't tell you.\n",
      "What was he like?\n",
      "Your pardon, my lordsy means men fighting, Jon Snow.\n",
      "A girl and that's where you come in?\n",
      "And with little man must win at me,\n",
      "Or I'd be long need you.\n",
      "Those men, they said my father knew what they were to r most for you his subjects for the piece of pardon.\n",
      "You can't let her suffer.\n",
      "Once Oberyn gave me \"my lion.\n",
      "- You would not have my honour?\n",
      "I'll not sit meekly by... and wet will be all, Lord Arryn.\n",
      "give it to our vows hurry?\n",
      "You gave him the riv\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"Seven Hells\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: [Text generation with an RNN](https://www.tensorflow.org/tutorials/text/text_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
