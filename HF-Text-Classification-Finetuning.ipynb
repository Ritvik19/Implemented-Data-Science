{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.metrics import precision_recall_fscore_support, confusion_matrix\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\npd.options.display.max_columns = 30","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-27T09:39:42.792321Z","iopub.execute_input":"2022-06-27T09:39:42.792902Z","iopub.status.idle":"2022-06-27T09:39:50.365832Z","shell.execute_reply.started":"2022-06-27T09:39:42.792780Z","shell.execute_reply":"2022-06-27T09:39:50.364914Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Get the Data","metadata":{}},{"cell_type":"code","source":"x_train, y_train = fetch_20newsgroups(subset='train', return_X_y=True)\nx_valid, y_valid = fetch_20newsgroups(subset='test', return_X_y=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T09:39:50.367594Z","iopub.execute_input":"2022-06-27T09:39:50.368248Z","iopub.status.idle":"2022-06-27T09:39:59.030330Z","shell.execute_reply.started":"2022-06-27T09:39:50.368210Z","shell.execute_reply":"2022-06-27T09:39:59.029322Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Initialise Model","metadata":{}},{"cell_type":"code","source":"checkpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=20)\ntraining_args = TrainingArguments(\"news_classifier\", num_train_epochs=2)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T09:39:59.031885Z","iopub.execute_input":"2022-06-27T09:39:59.032253Z","iopub.status.idle":"2022-06-27T09:40:22.118095Z","shell.execute_reply.started":"2022-06-27T09:39:59.032215Z","shell.execute_reply":"2022-06-27T09:40:22.117110Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f33df9326934ccca1be6632a0cf7645"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5532af5f7de04e7688479314d03e65db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"344198d077bc47579ae9ed0b6bf376ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18ea5be4ea2644fba7501c072dd8900f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acbee97447cc421a89d11884b0cb7f97"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Tokenize Data","metadata":{}},{"cell_type":"code","source":"def tokenize(texts):\n    return tokenizer(\n        texts, padding='max_length', truncation=True, max_length=512, return_tensors='pt'\n    )","metadata":{"execution":{"iopub.status.busy":"2022-06-27T09:40:22.121099Z","iopub.execute_input":"2022-06-27T09:40:22.121710Z","iopub.status.idle":"2022-06-27T09:40:22.127101Z","shell.execute_reply.started":"2022-06-27T09:40:22.121668Z","shell.execute_reply":"2022-06-27T09:40:22.126053Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"x_train_tokenized = tokenize(x_train)\nx_valid_tokenized = tokenize(x_valid)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T09:40:22.129153Z","iopub.execute_input":"2022-06-27T09:40:22.129610Z","iopub.status.idle":"2022-06-27T09:40:48.253933Z","shell.execute_reply.started":"2022-06-27T09:40:22.129568Z","shell.execute_reply":"2022-06-27T09:40:48.252890Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Data Loaders","metadata":{}},{"cell_type":"code","source":"class TextClassificationDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = TextClassificationDataset(x_train_tokenized, y_train)\nvalid_dataset = TextClassificationDataset(x_valid_tokenized, y_valid)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T09:40:48.255863Z","iopub.execute_input":"2022-06-27T09:40:48.256277Z","iopub.status.idle":"2022-06-27T09:40:48.265838Z","shell.execute_reply.started":"2022-06-27T09:40:48.256212Z","shell.execute_reply":"2022-06-27T09:40:48.264803Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(\n    model,\n    training_args,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T09:40:48.267163Z","iopub.execute_input":"2022-06-27T09:40:48.267839Z","iopub.status.idle":"2022-06-27T10:02:15.690036Z","shell.execute_reply.started":"2022-06-27T09:40:48.267800Z","shell.execute_reply":"2022-06-27T10:02:15.688779Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 11314\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2830\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.12.19 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.18"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20220627_094100-361heuga</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/ritvik19/huggingface/runs/361heuga\" target=\"_blank\">news_classifier</a></strong> to <a href=\"https://wandb.ai/ritvik19/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  import sys\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2830' max='2830' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2830/2830 21:10, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.318600</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.659100</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.477400</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.246700</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.232900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to news_classifier/checkpoint-500\nConfiguration saved in news_classifier/checkpoint-500/config.json\nModel weights saved in news_classifier/checkpoint-500/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  import sys\nSaving model checkpoint to news_classifier/checkpoint-1000\nConfiguration saved in news_classifier/checkpoint-1000/config.json\nModel weights saved in news_classifier/checkpoint-1000/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  import sys\nSaving model checkpoint to news_classifier/checkpoint-1500\nConfiguration saved in news_classifier/checkpoint-1500/config.json\nModel weights saved in news_classifier/checkpoint-1500/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  import sys\nSaving model checkpoint to news_classifier/checkpoint-2000\nConfiguration saved in news_classifier/checkpoint-2000/config.json\nModel weights saved in news_classifier/checkpoint-2000/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  import sys\nSaving model checkpoint to news_classifier/checkpoint-2500\nConfiguration saved in news_classifier/checkpoint-2500/config.json\nModel weights saved in news_classifier/checkpoint-2500/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  import sys\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2830, training_loss=0.5438325106887009, metrics={'train_runtime': 1281.7669, 'train_samples_per_second': 17.654, 'train_steps_per_second': 2.208, 'total_flos': 5954639162621952.0, 'train_loss': 0.5438325106887009, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"model_checkpoint = 'news_classifier/checkpoint-2500/'\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=20)\ntrainer = Trainer(\n    model,\n    training_args\n)\npredictions = trainer.predict(valid_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T10:02:15.691668Z","iopub.execute_input":"2022-06-27T10:02:15.692062Z","iopub.status.idle":"2022-06-27T10:04:30.475018Z","shell.execute_reply.started":"2022-06-27T10:02:15.692014Z","shell.execute_reply":"2022-06-27T10:04:30.470914Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"loading configuration file news_classifier/checkpoint-2500/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"news_classifier/checkpoint-2500/\",\n  \"architectures\": [\n    \"BertForSequenceClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\",\n    \"4\": \"LABEL_4\",\n    \"5\": \"LABEL_5\",\n    \"6\": \"LABEL_6\",\n    \"7\": \"LABEL_7\",\n    \"8\": \"LABEL_8\",\n    \"9\": \"LABEL_9\",\n    \"10\": \"LABEL_10\",\n    \"11\": \"LABEL_11\",\n    \"12\": \"LABEL_12\",\n    \"13\": \"LABEL_13\",\n    \"14\": \"LABEL_14\",\n    \"15\": \"LABEL_15\",\n    \"16\": \"LABEL_16\",\n    \"17\": \"LABEL_17\",\n    \"18\": \"LABEL_18\",\n    \"19\": \"LABEL_19\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_10\": 10,\n    \"LABEL_11\": 11,\n    \"LABEL_12\": 12,\n    \"LABEL_13\": 13,\n    \"LABEL_14\": 14,\n    \"LABEL_15\": 15,\n    \"LABEL_16\": 16,\n    \"LABEL_17\": 17,\n    \"LABEL_18\": 18,\n    \"LABEL_19\": 19,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3,\n    \"LABEL_4\": 4,\n    \"LABEL_5\": 5,\n    \"LABEL_6\": 6,\n    \"LABEL_7\": 7,\n    \"LABEL_8\": 8,\n    \"LABEL_9\": 9\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"problem_type\": \"single_label_classification\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file news_classifier/checkpoint-2500/pytorch_model.bin\nAll model checkpoint weights were used when initializing BertForSequenceClassification.\n\nAll the weights of BertForSequenceClassification were initialized from the model checkpoint at news_classifier/checkpoint-2500/.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n***** Running Prediction *****\n  Num examples = 7532\n  Batch size = 8\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  import sys\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='942' max='942' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [942/942 02:12]\n    </div>\n    "},"metadata":{}}]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"clf_report = pd.DataFrame(confusion_matrix(y_valid, predictions.label_ids))\nprecision, recall, fscore, support = precision_recall_fscore_support(y_valid, predictions.label_ids)\nclf_report['precision'] = precision\nclf_report['recall'] = recall\nclf_report['fscore'] = fscore\nclf_report['support'] = support\nclf_report","metadata":{"execution":{"iopub.status.busy":"2022-06-27T10:04:30.476392Z","iopub.execute_input":"2022-06-27T10:04:30.476733Z","iopub.status.idle":"2022-06-27T10:04:30.572488Z","shell.execute_reply.started":"2022-06-27T10:04:30.476696Z","shell.execute_reply":"2022-06-27T10:04:30.571149Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"      0    1    2    3    4    5    6    7    8    9   10   11   12   13   14  \\\n0   319    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n1     0  389    0    0    0    0    0    0    0    0    0    0    0    0    0   \n2     0    0  394    0    0    0    0    0    0    0    0    0    0    0    0   \n3     0    0    0  392    0    0    0    0    0    0    0    0    0    0    0   \n4     0    0    0    0  385    0    0    0    0    0    0    0    0    0    0   \n5     0    0    0    0    0  395    0    0    0    0    0    0    0    0    0   \n6     0    0    0    0    0    0  390    0    0    0    0    0    0    0    0   \n7     0    0    0    0    0    0    0  396    0    0    0    0    0    0    0   \n8     0    0    0    0    0    0    0    0  398    0    0    0    0    0    0   \n9     0    0    0    0    0    0    0    0    0  397    0    0    0    0    0   \n10    0    0    0    0    0    0    0    0    0    0  399    0    0    0    0   \n11    0    0    0    0    0    0    0    0    0    0    0  396    0    0    0   \n12    0    0    0    0    0    0    0    0    0    0    0    0  393    0    0   \n13    0    0    0    0    0    0    0    0    0    0    0    0    0  396    0   \n14    0    0    0    0    0    0    0    0    0    0    0    0    0    0  394   \n15    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n16    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n17    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n18    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n19    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n\n     15   16   17   18   19  precision  recall  fscore  support  \n0     0    0    0    0    0        1.0     1.0     1.0      319  \n1     0    0    0    0    0        1.0     1.0     1.0      389  \n2     0    0    0    0    0        1.0     1.0     1.0      394  \n3     0    0    0    0    0        1.0     1.0     1.0      392  \n4     0    0    0    0    0        1.0     1.0     1.0      385  \n5     0    0    0    0    0        1.0     1.0     1.0      395  \n6     0    0    0    0    0        1.0     1.0     1.0      390  \n7     0    0    0    0    0        1.0     1.0     1.0      396  \n8     0    0    0    0    0        1.0     1.0     1.0      398  \n9     0    0    0    0    0        1.0     1.0     1.0      397  \n10    0    0    0    0    0        1.0     1.0     1.0      399  \n11    0    0    0    0    0        1.0     1.0     1.0      396  \n12    0    0    0    0    0        1.0     1.0     1.0      393  \n13    0    0    0    0    0        1.0     1.0     1.0      396  \n14    0    0    0    0    0        1.0     1.0     1.0      394  \n15  398    0    0    0    0        1.0     1.0     1.0      398  \n16    0  364    0    0    0        1.0     1.0     1.0      364  \n17    0    0  376    0    0        1.0     1.0     1.0      376  \n18    0    0    0  310    0        1.0     1.0     1.0      310  \n19    0    0    0    0  251        1.0     1.0     1.0      251  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>fscore</th>\n      <th>support</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>319</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>319</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>389</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>389</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>394</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>394</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>392</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>392</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>385</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>385</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>395</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>395</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>390</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>390</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>396</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>396</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>398</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>398</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>397</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>397</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>399</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>399</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>396</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>396</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>393</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>393</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>396</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>396</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>394</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>394</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>398</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>398</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>364</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>364</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>376</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>376</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>310</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>310</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>251</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>251</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}